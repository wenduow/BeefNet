\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{boden2001guide}
\citation{fahlman1988empirical}
\citation{riedmiller1993direct}
\citation{hagan1994training}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{nissen2003implementation}
\citation{lopezopennn}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Computational Framework for Implementing Neural Learning Algorithms on Multicore machines}{2}{section.2}}
\newlabel{section:implementation}{{2}{2}{A Computational Framework for Implementing Neural Learning Algorithms on Multicore machines}{section.2}{}}
\citation{chu2007map}
\citation{schuessler2011parallel}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of multithread implementation of neural learning process.}}{3}{figure.1}}
\newlabel{fig:parallelization}{{1}{3}{Illustration of multithread implementation of neural learning process}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Parallelization}{3}{subsection.2.1}}
\citation{wiki:generic_programming}
\citation{alexandrescu2001preface}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Abstraction of Weight, Neuron, Bias, Input and Target}{4}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 1-layer of neural network topology represented by the abstraction layers (abs-layer). The abstracted layers inside the box in dash-line is the neural network layer.}}{5}{figure.2}}
\newlabel{fig:nn_abstracted}{{2}{5}{1-layer of neural network topology represented by the abstraction layers (abs-layer). The abstracted layers inside the box in dash-line is the neural network layer}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Communication between nodes during feed-forward and back-propagation.}}{5}{figure.3}}
\newlabel{fig:microscopic}{{3}{5}{Communication between nodes during feed-forward and back-propagation}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Compile-Time Generalization to Learning Algorithms, Transfer Functions, Error Functions and Network Topologies}{5}{subsection.2.3}}
\citation{alexandrescu2001policy}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Run-time generalization versus compile-time generalization during the processes of compiling code to running code for a neural learning algorithm. Note $n$ is epoch number and $m$ is number of experiment repeated.}}{6}{figure.4}}
\newlabel{fig:run_vs_compile}{{4}{6}{Run-time generalization versus compile-time generalization during the processes of compiling code to running code for a neural learning algorithm. Note $n$ is epoch number and $m$ is number of experiment repeated}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Flexibility and Reusability of GPNN Framework}{6}{subsection.2.4}}
\citation{fahlman1988empirical}
\citation{riedmiller1993direct}
\citation{hagan1994training}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Building a recurrent neural network by modifying a 1-layer neural network.}}{7}{figure.5}}
\newlabel{fig:reusability}{{5}{7}{Building a recurrent neural network by modifying a 1-layer neural network}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Performance and Experiment Result}{7}{section.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Features in climate dataset}}{8}{table.1}}
\newlabel{table:climate}{{1}{8}{Features in climate dataset}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multithread Efficiency}{8}{subsection.3.1}}
\citation{yu2011levenberg}
\citation{matlab:neural_networks}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training time of BP for different hidden node numbers versus different numbers of threads.}}{9}{figure.6}}
\newlabel{fig:thread_efficiency}{{6}{9}{Training time of BP for different hidden node numbers versus different numbers of threads}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Comparing GPNN Implementation with Conventional Implementation}{9}{subsection.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training time at different thread number}}{9}{table.2}}
\newlabel{table:algorithm_complexity}{{2}{9}{Training time at different thread number}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Algorithm Efficiency}{9}{subsection.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Converge epochs and errors of different algorithms}}{10}{table.3}}
\newlabel{table:algorithm_efficiency}{{3}{10}{Converge epochs and errors of different algorithms}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Neural network configuration for algorithm efficiency experiment}}{10}{table.4}}
\newlabel{table:config_algorithm_efficiency}{{4}{10}{Neural network configuration for algorithm efficiency experiment}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.4}}
\bibstyle{plain}
\bibdata{gpnn}
\bibcite{alexandrescu2001preface}{1}
\bibcite{alexandrescu2001policy}{2}
\bibcite{boden2001guide}{3}
\bibcite{chu2007map}{4}
\bibcite{fahlman1988empirical}{5}
\bibcite{hagan1994training}{6}
\bibcite{lopezopennn}{7}
\bibcite{matlab:neural_networks}{8}
\bibcite{nissen2003implementation}{9}
\bibcite{riedmiller1993direct}{10}
\bibcite{schuessler2011parallel}{11}
\bibcite{wiki:generic_programming}{12}
\bibcite{yu2011levenberg}{13}
\@writefile{toc}{\contentsline {section}{\numberline {A}Comparison with Other Neural Networks Library}{11}{appendix.A}}
\newlabel{appendix:comparison}{{A}{11}{Comparison with Other Neural Networks Library}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Neural network library characteristics}}{11}{table.5}}
\newlabel{table:library_compare}{{5}{11}{Neural network library characteristics}{table.5}{}}
\newlabel{LastPage}{{}{11}{}{page.11}{}}
\xdef\lastpage@lastpage{11}
\xdef\lastpage@lastpageHy{11}
