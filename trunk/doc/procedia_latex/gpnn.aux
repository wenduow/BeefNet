\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{boden2001guide}
\citation{fahlman1988empirical}
\citation{riedmiller1993direct}
\citation{hagan1994training}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{nissen2003implementation}
\citation{lopezopennn}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Computational Framework for Implementing Neural Learning Algorithms on Multicore machines}{2}{section.2}}
\citation{chu2007map}
\citation{schuessler2011parallel}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Multithread training procedure is a modification of traditional batch mode training. Weights will copy to several network images. Each image will feed-forward its training patterns and back-propagate gradients. Gradients of different network images will then merged together to be updated. The whole runs in a loop until stop criteria is met.}}{3}{figure.1}}
\newlabel{fig:parallelization}{{1}{3}{Multithread training procedure is a modification of traditional batch mode training. Weights will copy to several network images. Each image will feed-forward its training patterns and back-propagate gradients. Gradients of different network images will then merged together to be updated. The whole runs in a loop until stop criteria is met}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Parallelization}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Abstraction of Weight, Neuron, Bias, Input and Target}{4}{subsection.2.2}}
\citation{wiki:generic_programming}
\citation{alexandrescu2001preface}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 1-layer neural network topology with abstracted layer (abs-layer) interpretation. The abstracted layers inside the dash-line compose a classic layer.}}{5}{figure.2}}
\newlabel{fig:nn_abstracted}{{2}{5}{1-layer neural network topology with abstracted layer (abs-layer) interpretation. The abstracted layers inside the dash-line compose a classic layer}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A microscopic view of node. It connects the outputs and inputs of other nodes.}}{5}{figure.3}}
\newlabel{fig:microscopic}{{3}{5}{A microscopic view of node. It connects the outputs and inputs of other nodes}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Compile-Time Generalization to Learning Algorithms, Transfer Functions, Error Functions and Network Topologies}{5}{subsection.2.3}}
\citation{alexandrescu2001policy}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Run-time generalization versus compile-time generalization from compiling code to running code. $n$ is epoch number and $m$ is experiment running times.}}{6}{figure.4}}
\newlabel{fig:run_vs_compile}{{4}{6}{Run-time generalization versus compile-time generalization from compiling code to running code. $n$ is epoch number and $m$ is experiment running times}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Scalability and Reusability}{7}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Modification from a 1-layer neural network to a recurrent neural network.}}{7}{figure.5}}
\newlabel{fig:reusability}{{5}{7}{Modification from a 1-layer neural network to a recurrent neural network}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Scalable Possibilities}}{7}{table.1}}
\newlabel{table:scalability}{{1}{7}{Scalable Possibilities}{table.1}{}}
\citation{riedmiller1993direct}
\citation{fahlman1988empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Possible weight update trends, includes convergence (left), oscillation (middle) and divergence (right). The solid curve represents error vs. weight, local minimum is at the intersection between the solid curve and the dash line, red arrows represent weight update with positive gradient, and green arrows represent weight update with negative gradient.}}{8}{figure.6}}
\newlabel{fig:bp}{{6}{8}{Possible weight update trends, includes convergence (left), oscillation (middle) and divergence (right). The solid curve represents error vs. weight, local minimum is at the intersection between the solid curve and the dash line, red arrows represent weight update with positive gradient, and green arrows represent weight update with negative gradient}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Application on Neural Learning Algorithms}{8}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Classic Back-Propagation (BP)}{8}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Quick Propagation (QP)}{8}{subsection.3.2}}
\citation{fahlman1988empirical}
\citation{riedmiller1993direct}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Gradient (blue solid parabola) and its first derivative (red solid line). Minimum error is reached at which the first derivative equals to zero.}}{9}{figure.7}}
\newlabel{fig:qp}{{7}{9}{Gradient (blue solid parabola) and its first derivative (red solid line). Minimum error is reached at which the first derivative equals to zero}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Resilient Propagation (RP)}{9}{subsection.3.3}}
\citation{riedmiller1993direct}
\citation{hagan1994training}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces When gradient doesn’t change its sign (left), weight takes a larger step by a ratio $\eta ^ + > 1$ to update. When gradient changes its sign (right), weight doesn’t update at this epoch but will take a smaller step by a ratio $0 < \eta ^ - < 1$ to update at next epoch.}}{10}{figure.8}}
\newlabel{fig:rp}{{8}{10}{When gradient doesn’t change its sign (left), weight takes a larger step by a ratio $\eta ^ + > 1$ to update. When gradient changes its sign (right), weight doesn’t update at this epoch but will take a smaller step by a ratio $0 < \eta ^ - < 1$ to update at next epoch}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Levenberg-Marquardt Algorithm (LM)}{10}{subsection.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Performance and Experiment Result}{11}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multithread Efficiency}{11}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Features in climate dataset}}{12}{table.2}}
\newlabel{table:climate}{{2}{12}{Features in climate dataset}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Training time for different hidden node number using different thread number}}{12}{table.3}}
\newlabel{table:thread_efficiency}{{3}{12}{Training time for different hidden node number using different thread number}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Algorithm Complexity}{12}{subsection.4.2}}
\citation{yu2011levenberg}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training time for different hidden node number configuration versus different numbers of threads. Curves from bottom to top corresponding to the network hidden node number equals to 10, 20, 30, 40 and 50.}}{13}{figure.9}}
\newlabel{fig:thread_efficiency}{{9}{13}{Training time for different hidden node number configuration versus different numbers of threads. Curves from bottom to top corresponding to the network hidden node number equals to 10, 20, 30, 40 and 50}{figure.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Neural network configuration for multithread efficiency experiment}}{13}{table.4}}
\newlabel{table:config_thread_efficiency}{{4}{13}{Neural network configuration for multithread efficiency experiment}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Training time at different thread number}}{13}{table.5}}
\newlabel{table:algorithm_complexity}{{5}{13}{Training time at different thread number}{table.5}{}}
\citation{matlab:neural_networks}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Neural network configuration for algorithm complexity experiment}}{14}{table.6}}
\newlabel{table:config_algorithm_complexity}{{6}{14}{Neural network configuration for algorithm complexity experiment}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Algorithm Efficiency}{14}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Converge epochs and errors of different algorithms}}{14}{table.7}}
\newlabel{table:algorithm_efficiency}{{7}{14}{Converge epochs and errors of different algorithms}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{14}{section.5}}
\bibstyle{plain}
\bibdata{my_ref}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Neural network configuration for algorithm efficiency experiment}}{15}{table.8}}
\newlabel{table:config_algorithm_efficiency}{{8}{15}{Neural network configuration for algorithm efficiency experiment}{table.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Comparison with Other Neural Networks Library}{15}{appendix.A}}
\newlabel{appendix:comparison}{{A}{15}{Comparison with Other Neural Networks Library}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Neural network library characteristics}}{15}{table.9}}
\newlabel{table:library_compare}{{9}{15}{Neural network library characteristics}{table.9}{}}
\newlabel{LastPage}{{}{15}{}{page.15}{}}
\xdef\lastpage@lastpage{15}
\xdef\lastpage@lastpageHy{15}
