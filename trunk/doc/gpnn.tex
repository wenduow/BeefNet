% easychair.tex,v 3.1 2011/12/30
%
% Select appropriate paper format in your document class as
% instructed by your conference organizers. Only withtimes
% and notimes can be used in proceedings created by EasyChair
%
% The available formats are 'letterpaper' and 'a4paper' with
% the former being the default if omitted as in the example
% below.
%
\documentclass[procedia]{easychair}
%\documentclass[debug]{easychair}
%\documentclass[verbose]{easychair}
%\documentclass[notimes]{easychair}
%\documentclass[withtimes]{easychair}
%\documentclass[a4paper]{easychair}
%\documentclass[letterpaper]{easychair}

% This provides the \BibTeX macro
\usepackage{doc}
\usepackage{makeidx}
\usepackage{multirow}
\usepackage{array}

% In order to save space or manage large tables or figures in a
% landcape-like text, you can use the rotating and pdflscape
% packages. Uncomment the desired from the below.
%
% \usepackage{rotating}
% \usepackage{pdflscape}

% If you plan on including some algorithm specification, we recommend
% the below package. Read more details on the custom options of the
% package documentation.
%
% \usepackage{algorithm2e}

% Some of our commands for this guide.
%
\newcommand{\easychair}{\textsf{gpnn}}
\newcommand{\miktex}{MiK{\TeX}}
\newcommand{\texniccenter}{{\TeX}nicCenter}
\newcommand{\makefile}{\texttt{Makefile}}
\newcommand{\latexeditor}{LEd}

\def\procediaConference{99th Conference on Topics of
  Superb Significance (COOL 2014)}

%\makeindex

%% Front Matter
%%
% Regular title as in the article class.
%
\title{A Computational Framework for Implementation of\\
       Neural Networks on Multi-Core Machine}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{Computational Framework for Implementation of Neural Networks}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes into the list
% defined using \institute
%
\author{
    Wenduo Wang\inst{1}
\and
    Yi L. Murphey\inst{1}
\and
    Paul Watta\inst{1}
}

% Institutes for affiliations are also joined by \and,
\institute{
    University of Michigan-Dearborn,
    Dearborn, Michigan, U.S.A\\
    \email{wenduow@umich.edu}
    \email{yilu@umich.edu}
    \email{watta@umich.edu}
}

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Wang, Murphey and Watta}

\begin{document}

\maketitle

\keywords{back-propagation, neural networks, parallel computing, multi-core machine, generic programming}

\begin{abstract}

This paper presents a computational framework, GPNN, for efficient implementation of Back-Propagation based neural learning algorithms running on multi-core machines.  GPNN has three components: parallelization of neural learning, abstraction of network components, and compile-time generalization.  The parallelization component decomposes the back-propagation learning algorithm into two stages: forward propagation and back-propagation.  During forward propagation, training data are partitioned and distributed to K-threads, which run simultaneously multiple cores of a neural network system on a multi-core machine. The back-propagation process, which runs on a single thread, collects the errors generated from the K-threads and uses it to update the network weights.  The abstraction component models the inputs, biases, weights, and neurons as abstract nodes and abstract layers.  The compile-time generalization component uses a generic programming technique to make neural network components instantiated at compile-time, which further reduces execution time. Together these computational components make GPNN an efficient framework for fast implementation of back-propagation based neural learning algorithms, and provide flexibility and reusability for modifying neural network topologies.  The GPNN was applied to four different neural learning algorithms: classic back-propagation (BP), quick propagation (QP), resilient propagation (RP) and Levenberg-Marquardt (LM) algorithm. Experiments were conducted to evaluate the effectiveness of GPNN, and results show that the neural learning algorithms implemented in GPNN are more efficient than their respective functions provided by Matlab.

\end{abstract}

%------------------------------------------------------------------------------

\section{Introduction}

Artificial neural networks are widely used in a wide range of research areas and have been to be an effective computational tool for many practical applications, especially those that require the discovery and extraction of features and knowledge from large and complex data sets.  However, as pointed by many researchers, current machine learning methodologies and software tools are not sufficient to handle the volume, variability, and velocity of big data \cite{fan2013mining, labrinidis2012challenges}.  Computational efficient algorithms are key areas of research in dealing with big data.  The most widely used software for processing big data is Apache Hadoop, which provides a set of algorithms for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.  Hadoop was initially designed to focus on running massive MapReduce jobs to process a web crawl.  Recently, as Hadoop has been used for a broader set of applications, some problems have emerged. \cite{vavilapalli2013apache}.  Another aspect of big data research involves fast implementation of neural learning algorithms such as FANN, OpenNN and tnnlib \cite{nissen2003implementation, lopezopennn}.  These implementations take advantage of generic programming, algorithm and topology diversity and parallel execution. Many important machine learning issues, for example, over-fitting, network size, memory constraints require the analysis of data generated from repeated and extensive experiments.

The research presented here focuses on (1) exploring the potential of using the latest software technology to maximize parallelism in the implementation of back-propagation based neural learning algorithms, and (2) developing a programming environment that is efficient and flexible enough to meet the challenges posed by big data.

In this paper, we present a computational framework, GPNN, designed for efficient implementation of the back-propagation based neural networks. The GPNN consists of three computational components, parallelization of neural learning, abstraction of neural network components: and compile-time generalization.  The parallelization component decomposes the back-propagation learning algorithm into two stages: forward propagation and back-propagation.  During the forward propagation stage, the training data are partitioned and distributed to $K$ threads, which all have identical images of the neural network.  In the back-propagation stage, which runs on a single thread, the errors generated from the $K$ threads are combined and used for weight update.  This parallelization technique can improve training time considerably for large training sets.

The abstraction component provides techniques for representing all of the neural network components, such as inputs, biases, weights, neurons and targets as abstract nodes and abstract layers.  The abstraction process provides an efficient architecture for implementing parallel computation in neural learning.  For example, since weights are the only data needed in the forward propagation state, they are represented in separate abstract layers so that they can be easily copied or shared among multiple threads as network images.  The remaining parts of the network, i.e. input nodes, biases, neurons and target nodes, can stay local without being distributed over the multi-cores.  Therefore the abstraction process helps to reduce communication time and required memory size, which could provide a significant improvement in efficiency when processing big data using large systems.  The Compile-Time Generalization component uses generic programming to instantiate data types such as learning algorithms, transfer functions, error functions, and network topologies at compile-time, which eliminates the need for the run-time type checking used in each loop.  Together these computational components in GPNN provides mechanisms for fast implementation of back-propagation based neural learning algorithms, flexibility and reusability for neural network research in studying neural network topologies, and a set of tools well suited for handling large data sets.

We have implemented four neural learning algorithms in the GPNN framework: classic back-propagation (BP) \cite{boden2001guide}, quick propagation (QP) \cite(fahlman1988empirical), resilient propagation (RP) \cite{riedmiller1993direct} and Levenberg-Marquardt (LM) algorithm \cite{hagan1994training}.  Experiments are conducted to evaluate the effectiveness of GPNN by applying these learning algorithms to solve a real-world problem: predicting humidity based on weather measures such as visibility, wind direction, dew point pressure, etc.  The run-time complexity of these algorithms are compared with the respective algorithms provided by Matlab.

This paper is organized as follows.  Section~\ref{section:implementation} introduces the three major components in GPNN, Section~\ref{section:performance} presents the experiment results, and in Section~\ref{section:conclusion}, we present our conclusions and outline future work.

%------------------------------------------------------------------------------

\section{A Computational Framework for Implementing Back-Propagation based Neural Learning Algorithms}
\label{section:implementation}

The proposed computational framework, GPNN, is developed to optimize the implementation of back-propagation based neural networks.  Back-Propagation is the most popular neural learning algorithm for supervised learning in multi-layered feed-forward networks, as well as in many recurrent neural networks have two computational paths: a \textit{forward path} and a \textit{back-propagation path}.

The forward path is used to propagate the input from the input layer to the output layer.  Suppose we have a single hidden layer neural network of size $n \times J \times K$ nodes, where the input is denoted $\textbf{x} = [x_1,x_2,...,x_n]^\top$, the output is given by $\textbf{y} = [y_1,y_2,...,y_k]^\top$, and the hidden unit outputs are $\textbf{z}=[z_1,z_2,...,z_J]^\top$.  Let the hidden layer weight matrix be denoted $\textbf{V}$ (of size $J \times n$) and the output layer weight matrix be denoted $\textbf{W}$ (of size $K \times J$).  Then the forward path computation is given by:

\begin{equation}
    \textbf{y}=\textbf{F}_o(\textbf{W}\textbf{F}_h(\textbf{Vx})) \notag
\end{equation}

where $\textbf{F}_o$ is a vector mapping which contains all of the output unit activation functions: $\textbf{F}_o=[f_1,f_2,...,f_k]^\top$ and $\textbf{F}_h$ contains all of the output unit activation functions: $\textbf{F}_h=[f_1,f_2,...,f_J]^\top$.  If we have a training set contains $m$ patterns:

\begin{equation}
    TS = {(\textbf{x}_1,\textbf{t}_1),(\textbf{x}_2,\textbf{t}_2),...,(\textbf{x}_m,\textbf{t}_m)} \notag
\end{equation}

then the sum-squared error over the training set is given by:

\begin{equation}
    E = \frac{1}{2} \sum_{i=1}^m {||\textbf{y}_i-\textbf{t}_i||}^2 \notag
\end{equation}

In the backward path, the weights are updated to minimize the error function (following gradient descent):

\begin{gather}
    \Delta w_{kj} = \delta_j y_k \notag \\
    \Delta v_{ji} = f'(net_j) \sum_j {\delta_j w_{kj}} \notag
\end{gather}

In software implementation, a classic neural network layer is normally modeled as a container, which holds a weight matrix, a bias node and multiple neurons.  Additionally, it specifies the propagation order of these components.  In a global view, layer can be connected with each other.

The following introduces the three major components in GPNN, parallelization of neural learning, abstraction of neural network components, and compile-time generalization.  We will also show, using an example, the flexibility of reconstruct a new neural network from an existing one in the GPNN framework.

\subsection{Parallelization of Neural Learning Algorithm}

According to Moore’s law, the computational power of computers doubles every 18-24 months \cite{chu2007map}.  In the recent years, computational power has not been increased by every faster CPU clock rates; rather, computer systems have increased the number of cores significantly to support parallel computing.  Particularly in big data applications, it is important to implement neural learning algorithms on systems which can achieve a high level of parallelization, such as multi-core CPUs with shared memory.  We propose a multi-threaded implementation of batch training BP \cite{schuessler2011parallel}, which is illustrated in Fig.~\ref{fig:parallelization}.  Here, a $K$-thread parallel computing structure is used.  The neural network is copies $K$ times, and the training data are partitioned and distributed to these $K$ threads.  Training inputs are processed simultaneously along the feed-forward paths of these $K$ threads, and errors are collected at the end of each thread.  Then the errors of all $K$ threads are combined for weight update using gradient descent (back-propagation path), which is represented as follows,

\begin{equation}
    \frac{\partial E_j}{\partial w_{ji}} = \sum_k \frac{\partial E_j^{(k)}}{\partial w_{ji}^{(k)}} \text{.} \notag
\end{equation}

Weights are then all updated through the back-propagation process.  The updated neural network is again copied $K$ times, and the same training process is repeated until the user defined stop criteria are satisfied.

\begin{figure}[tb]
    \begin{centering}
        \includegraphics[scale=0.5]{../pic/parallelization.png}
        \caption{System diagram of multi-thread implementation of neural learning.}
    \label{fig:parallelization}
	\end{centering}
\end{figure}

\subsection{Abstraction of Neural Network Components}

Abstraction is a very critical and powerful concept in object-oriented programming.  It treats objects with similar functions as the same module.  Based on this concept, weights between neurons can be considered as similar to neurons, since a neuron has one input axon and one output axon, and a weight connecting two neurons can also be considered to have an input, which is the output axon of the preceding neuron, and the weight can be considered as its output axon, which is connected to the input axon of following neuron.  The transfer function of a weight is defined as follows,

\begin{align}
    & \text{feed-forward:} & x_{ji} = f_{ji}(x_i) = w_{ji}x_i \notag \\
    & \text{back-propagation:} & \delta_j = w_{ji}\delta_{ji} \text{.} \notag
\end{align}

For the same reason, an input node to a neural network can also be treated as similar to a neuron, which has the equal number of output axons to the first hidden layer but no input axon.  Similarly, bias of a neuron layer can also be handled in this way.  There is no need for back-propagation from any of these input nodes.

\begin{align}
	& \text{feed-forward for input:} & x_i = f_0(p) = p \notag \\
	& \text{feed-forward for bias:} & x_b = f_b(1) = 1 \notag
\end{align}

where $p$ is the value of input feature, $x_i$ is the output of the input node, and the output of the bias node $x_b$ is always 1.

The output of a neuron is represented as a target node, which has one input axon connected to the output of a neural network, but no output axon, i.e., there is no feed-forward from a target node.

\begin{align}
    & \text{target node:} & \delta_j = t_j - y_j \notag
\end{align}

where $t_j$ is the target value of this output node, and $y_j$ is the network predicted value.  Target nodes are used in back-propagation process.

Based on the above discussion, the network inputs, biases, weights, neurons and targets are all considered as nodes in this abstraction context.  Nodes of the same type are grouped into the same abstraction layer, and these abstracted layers are connected to each other as illustrated in Fig.~\ref{fig:nn_abstracted}, in which one-layer of neurons is represented by input abs-layer (abstraction layer), bias abs-layer, weight abs-layer, neuron abs-layer and target node abs-layer.  The connection among nodes and abstracted layer are considered equivalent in the context of programming.

\begin{figure}[tb]
    \begin{centering}
        \includegraphics[scale=0.5]{../pic/nn_abstracted.png}
        \caption{A one-layer of neural network represented by the abstraction layers (abs-layer).  The interior of the dash line indicates the layer.}
        \label{fig:nn_abstracted}
	\end{centering}
\end{figure}

The abstraction process provides an efficient architecture for implementing parallel computations in neural learning, since only weights need to be copied or shared among multiple threads as network images.  In the abstraction process, weights are represented in separate abstract layers so they can be easily copied or shared without touching the other parts of the network, e.g., input nodes, biases, neurons and target nodes, which can stay local without being distributed over the multi-cores.  Therefore the abstraction approach can reduce communication cost, and reduce the need for memory.  This improvement could be significant in applications involving neural learning from big data.

Current input and output of a node are stored for each input pattern for weight update processing during the back-propagation.  Other values are prepared during feed-forward or back-propagation for intermediate calculation of critical variables, e.g., $f(net)$ and $f'(net)$ shown in Fig.~\ref{fig:microscopic}.

\begin{figure}[tb]
    \begin{centering}
        \includegraphics[scale=0.5]{../pic/microscopic.png}
        \caption{Communication between nodes during feed-forward and back-propagation.}
        \label{fig:microscopic}
	\end{centering}
\end{figure}

\subsection{Compile-Time Generalization}

Generic programming is a software approach that can be used to generalize replaceable functional nodes in neural networks.   In a generic programming architecture, variables can be written in terms of types to-be-specified-later  \cite{wiki:generic_programming}, and then instantiated when needed.  In neural networks, transfer functions, network topologies, and weight update functions can be type-deduced so that functional calls can be determined at compile time  \cite{alexandrescu2001preface}.

We proposed to implement weights using compile-time generalization, since they are used in different data types including thee forward, backward, update and copy processes.  The user can specify an appropriate update strategy of weights during programming without modifying the rest part of the code.  The compiler then generates a made-to-order target file related to the customized behavior data types.  In addition, since the software is tailored to specific behavior data types at compile time, this implementation greatly reduces irrelevant code, thus, reducing the software size of the compiled code.   The compile-time generalization of weights eliminates the run-time checking that would otherwise be needed in every loop to determine the type of each weight object (through looking up its virtual table).

Fig.~\ref{fig:run_vs_compile} illustrates the differences between run-time and compile-time implementation of a neural learning process.  In the run-time generation implementation, the weight type is deduced in every epoch, which means that the processors need to spend time on deciding weight type in each loop through looking up a virtual table.  As a consequence, the cumulative time consumed in all loops is tremendous.  With the compile-time generalization, data type deduction is calculated only once at the compiling process.  Moreover, if the neural network software is required to be run multiple times for different experimental purposes, run-time generalization time on type deduction can be more significant.  Because there’s no need to compile the same neural network software again for repeated experiments, the compile-time generalization does not need extra time for data type deduction.  In summary, the time complexity for run-time generalization of weights is $O(mn)$, which can be saved if data type deduction for weights are carried out at the compiling process.

\begin{figure}[tb]
    \begin{centering}
        \includegraphics[scale=0.5]{../pic/run_vs_compile.png}
        \caption{Run-time generalization versus compile-time generalization during the processes of compiling code for a neural learning algorithm, where $n$ is epoch number and $m$ is the number of repeated experiments.}
        \label{fig:run_vs_compile}
	\end{centering}
\end{figure}

In terms of design patterns, compile-time generalization is also known as policy based class design \cite{alexandrescu2001policy}.  In our implementation, a learning algorithm is defined as a type of update policy, a transfer function is defined as a type of transfer policy, a network topology is defined as a kind of topology policy, an error function is defined as a kind of error policy, and even the number of neurons, input, target and other training factors can be considered as individual policies as well.

\subsection{Flexibility and Reusability of GPNN Framework}

In addition to the fast implementation of neural learning algorithms, the proposed GPNN framework also provides flexibility and reusability for neural network research.  The abstraction and compile-time generalization processes in GPNN allow researchers to build new neural networks by simply connecting or pruning nodes, without the need of re-constructing most of an existing network architecture.  The following algorithm shows the steps to build a recurrent neural network, which has no bias, and uses the LM algorithm and log-sigmoid transfer function, based on an existing biased one-layer neural network, which uses the BP algorithm and a linear transfer function.  Fig.~\ref{fig:reusability} illustrates the algorithm.

\textit{Building a recurrent neural network}

\begin{enumerate}
    \item Detach the bias abstracted layer and the weight abstracted layer in the given neural network.
    \item Attach a neuron abstracted layer using linear transfer function, and attach a weight abstracted layer using LM algorithm.
    \item Replace the algorithm type of the weight abstracted layer from BP to LM, and replace the transfer function type of the neuron abstracted layer from log-sigmoid to linear.
\end{enumerate}

\begin{figure}[h]
    \begin{centering}
        \includegraphics[scale=0.5]{../pic/reusability.png}
        \caption{Building a recurrent neural network by modifying a one-layer neural network.}
        \label{fig:reusability}
	\end{centering}
\end{figure}

%------------------------------------------------------------------------------

\section{Performance and Experiment Result}
\label{section:performance}

We applied the proposed computational framework, GPNN, to four back-propagation based neural learning algorithms: classic back-propagation (BP), quick propagation (QP), resilient propagation (RP) and Levenberg-Marquardt (LM) algorithm.

Quick Propagation takes the largest steps possible to reach local minima without overshooting.  The QP learning algorithm is based on the assumption that the error versus weight curve can be approximated by a parabola whose arms open upward \cite{fahlman1988empirical}, which means its second derivative is approximately linear with a positive slope k.  For a parabola curve, the minimum value is where its second vanishes.  In general, QP is efficient for large training data.

The basic idea of Resilient Propagation is that every time the gradient changes its sign, it indicates that the last update was so big that the error function skipped a local minimum.  Thus, the weight update (absolute value) needs to be reduced by a factor $ \eta ^ - $, where $ 0 < \eta ^ - < 1 $.  On the other hand, if the gradient remains the same sign as in the previous update, a larger weight update step can be taken, e.g. increasing the weight change by a factor $\theta$ can be increased by factor  $ \eta ^ + $, where $ \eta ^ + > 1 $.  The details of the RP algorithm can be found in \cite{riedmiller1993direct}.

The Levenberg-Marquardt algorithm aims at solving a non-linear least square problem.  It interpolates between the Gauss–Newton algorithm (GNA) and the method of gradient descent. The LM algorithm is more robust than the GNA \cite{hagan1994training}.

We implemented these four neural learning algorithms using the parallelization, abstraction and compiler-time generalization processes discussed in Section~\ref{section:implementation}.

In order to evaluate the performance of the proposed GPNN, we used a training set consisting of hourly historical climate data of Ann Arbor, MI, USA, over the period 2010 to 2013 (available from \url{www.wunderground.com}).  The data from year 2010-2012 were used as training data and year 2013 as testing data.  The total number of training samples is 26304, and the total number testing samples is 8760.  The training samples were partitioned evenly and distributed to all the threads, the number of which is determined by the configuration of each experiment.  The ten input features listed in Table~\ref{table:climate} were input to each neural network system.  The neural network output is the predicted humidity value.  All input and output are normalized to zero mean ($ \mu = 0 $)and unit standard deviation ($ \sigma = 1 $).

\begin{table}[htp]
    \centering
    \caption{Features in Climate Dataset.}
    \begin{tabular}{ c l c c }
        \hline \hline
        Usage & Feature & Valid Range & Unit \\
        \hline
        \multirow{10}{*}{input}
            & month & 1 - 12 & - \\
            & hour & 0 - 23 & - \\
            & temperature & -50 - 150 & \(^\circ\)F \\
            & dew point & -50 - 150 & \(^\circ\)F \\
            & pressure & 28 - 31 & inHg \\
            & visibility & 0 - 10 & mile \\
            & wind direction & 0 - 359 & \(^\circ\) \\
            & wind speed & 0 - 50 & mph \\
            & gust speed & 0 - 100 & mph \\
            & precipitation & 0 - 1.5 & in \\
        \hline
        target & humidity & 0 - 100 & \% \\
        \hline \hline
    \end{tabular}
    \label{table:climate}
\end{table}

A computer with a 2.3GHz quad-core 8-thread CPU, 8G RAM, and a 64-bit operating system, was used to run all the experiments presented below.

\subsection{Evaluating Multi-Thread Efficiency}

Theoretically, multiple processors and cores can support almost any number of threads running simultaneously.  However, since the communication between threads is usually implemented by a pooling approach, it takes a certain amount of time to synchronize all the image threads to the main network thread.  It is clear that the most efficient number of threads should be equal to the number of cores, since the running time of multi-threads on the same core will be more than the running time of single thread even with thread scheduling applied.

The first experiment was conducted to evaluate GPNN training time as a function of the number of threads used and the number of hidden nodes.  Fig.~\ref{fig:thread_efficiency} shows the training time for each of these neural network configurations running on different number of threads.  In the experiments, the stop criterion is to training until 2000 epoch was reached.  In order to get a robust measure, and every experiment was repeated 10 times and the average training time was presented.

\begin{figure}[tb]
    \centering
    \includegraphics[scale=0.6]{../pic/efficiency.png}
    \caption{Training time of BP for different hidden node numbers versus numbers of threads and hidden neurons.}
    \label{fig:thread_efficiency}
\end{figure}

The result demonstrates that the fastest thread configuration is indeed 8 threads, which is the hardware concurrency of the CPU.  In the case of 10 hidden units, using 8 threads for neural learning is about 3 times faster than using a single thread.  As the number of threads grows larger than 8, though, the training time increases, which is due to the time cost for scheduling.

\subsection{Comparing GPNN Implementation with Conventional Implementation}

In this experiment, we compare the efficiency of the neural learning algorithms implemented using GPNN with their respective Matlab versions in the NN toolbox, except for the RP algorithm, which is not available in Matlab.  In this experiment, the stop criterion is also 2000 epochs.  Every experiment was repeated 10 times and the mean training time is presented in Table~\ref{table:algorithm_complexity}.

\begin{table}[htp]
    \centering
    \caption{Training Time at Different Thread Number.}
    \begin{tabular}{ l c c c c }
        \hline \hline
        Traing Time (s) & BP & QP & RP & LM \\
        \hline
        8-thread GPNN & 56.8 & 62.4 & 69.7 & 263.9 \\
        1-thread GPNN & 176.7 & 167.9 & 192.3 & 337.7 \\
        1-thread Matlab & 649.1 & - & 678.1 & 1264.0 \\
        \hline \hline
    \end{tabular}
    \label{table:algorithm_complexity}
\end{table}

It is no surprise that the 8-thread implementations of BP, QP and RP neural learning algorithms are much more computationally efficient than their respective 1-thread implementations.  It is noted that the LM algorithm is more time consuming than the other three neural learning algorithms.  This is because LM algorithm requires the calculation of the inverse of a Hessian matrix at each iteration \cite{yu2011levenberg}.  Even the 8-thread implementation of the LM algorithm does not reduce execution time much, which is due to the added time cost for communication among threads.  In the 1-thread implementation, the algorithms implemented in GPNN are much more efficient than their respective functions provided by Matlab, the speed up for the GPNN implemented BP is 3.6, the GPNN implemented QP is 3.5 times, and the GPNN LM is 3.7.

\subsection{Algorithm Efficiency based on Dynamic Stopping Criteria}

In many cases, neural network training is terminated based on a number of dynamic criteria; for example, the gradient is close enough to 0, the gradient stops decreasing, etc.  In this experiment, we train the GPNN implemented with the BP, QP, RP and LM learning algorithms using the following stopping criteria: when the mean error is less than $ 10 ^ {-9} $ or the error has not changed for 6 epochs \cite{matlab:neural_networks}.  The parameters of the four neural networks are shown in Table~\ref{table:config_algorithm_efficiency}.  Each experiment is repeated 10 times and the average CPU times are presented in Table~\ref{table:algorithm_efficiency}.

\begin{table}[htp]
    \centering
    \caption{Neural Networks Configuration used in Algorithm Efficiency Analysis Experiments.}
    \begin{tabular}{ c c c c c }
        \hline \hline
        & BP & QP & RP & LM \\
        \hline
        Number of threads & 8 & 8 & 8 & 8 \\
        Learning rate & 0.5 & 0.5 & - & - \\
        Max grow factor & - & 1.75 & - & - \\
        Decrease factor & - & - & 0.5 & - \\
        Increase factor & - & - & 1.2 & - \\
        Initial update value & - & - & 0.1 & - \\
        $\lambda$ & - & - & - & 10 \\
        $\beta$ & - & - & - & 10 \\
        \hline \hline
    \end{tabular}
    \label{table:config_algorithm_efficiency}
\end{table}

\begin{table}[htp]
    \centering
    \caption{Training Time of Neural Learning Algorithms with Dynamic Stopping Criteria.}
    \begin{tabular}{ l c c c c }
        \hline \hline
        & BP & QP & RP & LM \\
        \hline
        Training time (s) & 3.5 & 35.1 & 28.2 & 31.1 \\
        Converge epoch & 43.2 & 976.4 & 866.7 & 97.2 \\
        Training error (\%) & 3.80 & 1.86 & 0.73 & 3.15 \\
        Testing error (\%) & 3.66 & 1.88 & 0.76 & 33.08 \\
        \hline \hline
    \end{tabular}
    \label{table:algorithm_efficiency}
\end{table}

The results show that the LM algorithm converges in significantly less epochs than QP and RP.  However, it takes more time to run per epoch.  Although RP takes more epochs to converge, its training and testing errors are distinctly smaller than others.  The BP has the least training time and the least number of epochs, but its training and test error are the highest.  Based on these results we can deduce that if an application requires high accuracy, GPNN implemented RP algorithm is a good fit to find the local minima, and if big data are involved, LM is a good learning algorithm to use since it converges in the least number of epochs.

%------------------------------------------------------------------------------

\section{Conclusion}
\label{section:conclusion}

We have presented a computational framework, GPNN, for efficient implementation of back-propagation based neural learning algorithms for big data learning. In GPNN, the forward path of a neural network learning algorithm is distributed to $K$-threads, errors are then combined, and the weights updated by a back-propagation process running in a single thread.   The abstraction component in GPNN implements the abstractions of weights, neurons, transfer functions, input and output in terms of abstraction nodes, and organizes the nodes into abstraction layers according to node type. The abstraction process provides an efficient computational environment for parallel computation during neural learning.  The GPNN also introduced a compile-time generalization technique to make as many neural network components instantiated at compile time as possible, which further reduces the execution time.

Experimental results show that the four algorithms implemented in GPNN are more efficient than their respective functions provided by Matlab.  Specifically, the speed up for the GPNN implemented BP is 3.6, the GPNN implemented RP is 3.5 times, and the GPNN LM is 3.7.  When implemented in 8-thread parallelization with a fixed number of epochs as the training stopping criterion, the GPNN BP has the largest speed up, 3.1, over 1-thread GPNN BP, and LM algorithm has the least speed up: 1.3.  When the multiple stopping criteria of minimum error and error convergence are used, the results show that (1) although GPNN RP takes more epochs to converge, its training and testing errors are decidedly smaller than all others, (2) GPNN LM converges in less epochs than GPNN QP and GPNN RP, but it takes more time to run each epoch, and (3) the GPNN BP has the least training time, but its training and test error are the larger than all the other three.

Based on these results we conclude that GPNN RP algorithm is recommended in cases where minimum error is required, while GPNN BP is a good candidate for big data applications.

In addition to efficient run-time, the proposed GPNN framework also provides flexibility and reusability for neural network research.  The abstraction and compile-time generalization processes in GPNN allow researchers to build new neural networks by simply connecting or pruning nodes without redesign most of an existing network architecture.

We have put the four neural learning algorithms implemented using the GPNN in a library available under LGPL license at \url{www.github.com/wenduow/BeefNet}.

%------------------------------------------------------------------------------

%\appendix

%% References with BibTeX database:

\bibliographystyle{plain}
\bibliography{gpnn}

\end{document}

% EOF

